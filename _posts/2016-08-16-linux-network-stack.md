---

layout: post
title:  "Linux协议栈--UDP协议的实现"
date:   2016-08-13 10:20:10
categories: network
tags: TCP/IP 网络栈 协议栈

---

* content
{:toc}

### 前言

本文是这篇文章的前半部分翻译：

[Queueing in the Linux Network Stack](https://www.coverfire.com/articles/queueing-in-the-linux-network-stack)


### Linux网络栈中的队列

网络数据包队列是任何网络栈或设备的核心组成部分。他使得异步模块之间可以通信并且可以提高性能但是它的副作用是增加延迟。
本文的目标是解释IP数据包在Linux网络栈的什么地方进入队列，新的减少延迟的机制是怎样工作的例如BQL操作，以及怎样控制缓冲区来减少延迟。

下面这张图会贯穿整片文章，并且会做不同的修改来描述特定的概念。

![figure_1_v2]({{"/css/pics/figure_1_v2.png"}})
图1-Linux网络栈中数据包传送路径中高度简化版的数据包队列

### 驱动程序队列（又名：环形缓冲区）

在IP协议栈和网卡（network interface controller - NIC）之间有一个驱动程序队列。这个队列是一个先进先出（FIFO）环形缓冲区 -- 可以把他理解为一个固定大小的缓冲区。
这个驱动程序队列不包含数据包而是由一个个描述符组成，这些描述符指向一个被称作套接字内核缓冲区（SKB）的数据结构，SKB是内核用来表示网络数据包的结构体，他应用于整个协议栈。

![figure_2_v2]({{"/css/pics/figure_2_v2.png"}})

这个环形缓冲区的数据输入是IP协议栈中完整的IP数据包序列。这些数据包可能是本地产生的或者是当网络设备开启了IP路由功能时从一个网卡上接受的数据包要被路由到另一个网卡上。
数据包通过IP协议栈加载到环形缓冲区队列，然后硬件驱动通过网卡上的数据总线将数据包发送出去。

环形缓冲区存在的意义是确保无论系统什么时候需要发送数据，网卡都能够立即处理。也就是给了IP协议栈一个缓存区使得数据入队和硬件操作可以异步进行。
一种替代方法是当物理媒介准备好发送数据时网卡请求IP协议栈发送数据。但是请求不能瞬间完成导致浪费发送时机从而降低了吞吐量。
相反的做法是当IP数据包准备好后协议栈等待硬件直到可以发送数据，这种方法也不好因为IP协议栈在等待时不能做其他工作。

### 协议栈中的巨型帧

大多数网卡有一个固定的最大传送单元（MTU），他是硬件设备能够传送的最大数据帧。对于以太网设备来说默认的MTU是1500字节，但是有些以太网络支持9000字节的巨型帧。
在IP协议栈中MTU可以作为硬件设备能够传输的最大数据包大小的限制。例如，如果一个应用程序发送了2000个字节包给TCP套接字，IP协议栈需要创建两个IP数据包来接受数据使每个IP数据包的大小在1500字节以内。
当发送大量数据时由于MTU的限制会创建大量小型数据包然后通过环形缓冲区发送出去。

为了避免队列中出现过量的数据包，Linux内核采用了几种措施：`TCP segmentation offload` (TSO), `UDP fragmentation offload` (UFO) 和 `generic segmentation offload` (GSO)。
所有的这些优化允许IP协议栈创建大于MTU的数据包。在IPv4中可以创建最大为65536字节的数据包。由于TSO和UFO措施，使得网卡硬件自身必须将大型数据包分割为可以在物理设备上传输的小型数据包。
如果网卡不支持数据包分割，在进入环形缓冲区之前由GSO来完成这项工作。

前面讲到环形缓冲区存放着固定数量的描述符，这些描述符指向不同大小的数据包。由于TSO, UFO 和 GSO允许大型数据包，但是这种优化有一个负作用就是大量增加了排列在环形缓冲区中的字节数。
图3描述了这种情况：

![figure_3_v2]({{"/css/pics/figure_3_v2.png"}})

本文接下来关注的是数据包的传输路径，但是值得一提的是Linux也收到过类似于TSO, UFO 和 GSO这样的优化方法。这些优化措施也是致力于减少数据包的开销。
尤其是`generic receive offload`（GRO）它允许网卡驱动将接受到的数据包组合成一个大的数据包然后在传送给IP协议栈。当转发数据包时为了维持IP数据包端到端的特性，GRO需要还原成原始的数据包。
但是GRO有一个副作用，在转发数据包时将大型数据包分解为小型数据包给发送端时会导致瞬间有多个数据包进入队列，这种微型的爆发式数据包增长会对网络延迟造成负面影响。

### 饥饿和延迟

尽管IP协议栈和硬件之间的队列是必要的并且有益的，但是它会带来两个问题：饥饿和延迟。

如果网卡驱动被唤醒从队列中取出数据来发送但是队列是空的，硬件就会错失发送时机从而降低了系统的吞吐量，这种情况叫做饥饿。
注意当系统没有任何数据需要传送时空队列是正常的，不是饥饿。避免饥饿的难题在于IP协议栈填充数据到队列中和硬件驱动从队列中取出数据是异步的。
更糟糕的是填充数据和取出数据所需要的时间是变化的，这取决于系统的负载和其他外部条件例如网络接口的物理介质。
例如，当系统非常繁忙时IP协议栈只能得到非常少量的机会去往缓冲区中添加数据包，这就增加了硬件在队列中取数据时队列中没有数据的情况。
由于这个原因大的缓冲区有利于减少饥饿的可能性并确保高吞吐量。

虽然一个大队列对于繁忙系统维持高吞吐量是必要的，但是也有一个缺点就是增大延迟。

![figure_4_v2]({{"/css/pics/figure_4_v2.png"}})

图4展示了环形缓冲区中填满了TCP数据包，蓝色部分是分散的数据包。队列的最后一个是VoIP或者某游戏的数据包（黄色的）。像VoIP或者游戏这样的交互应用程序通常会定时发送对延时非常敏感的短小数据包，
同时其他应用程序快速产生大量的大数据包。这种高频率的大数据包可以很快填满缓冲区导致交互程序的数据包被推迟发送。为了更好的描述这种情况做以下假设：

* 一个网络接口的最大传输速率是`5Mbit/s`或者`5000000bits/s`。
* 每个大数据包的大小是`1500`字节或者`12000 bits`。
* 每个交互应用程序的数据包是`500`字节。
* 队列最多容纳128个描述符。
* 队列中有127个大数据包和1个交互应用程序的数据包在队列的最后面。

根据上面的假设，传输127个大数据包需要的时间是`(127 * 12,000) / 5,000,000 = 0.304`秒（304微妙这基本等于ping包的延迟时间）。
这个延迟时间对于交互应用程序来说是可以接受的但是这并不真实的延迟时间--这个时间仅仅表示从队列中发送完这些数据包。就像前面提到的如果TSO, UFO 或者 GSO被使能，这些数据包的大小可能大于1500字节。
这就会导致更大的延迟。

过大的数据包导致延迟增大的问题称为`Bufferbloat`。更多的细节请看[Controlling Queue Delay ](http://queue.acm.org/detail.cfm?id=2209336)和[Bufferbloat](http://www.bufferbloat.net/)项目。

根据上面的讨论，选择一个合适大小的环形缓冲区是一个`度`的问题--不能太小会降低吞吐量，也不能太大会增大延迟时间。
